<!--
 * @Description: 
 * @Version: 1.0
 * @Autor: xihuishaw
 * @Date: 2022-01-07 23:42:54
 * @LastEditors: xihuishaw
 * @LastEditTime: 2022-01-08 00:34:48
-->

# 决策树

决策树学习的本质是，**从训练数据集中归纳出一组分类规则**。

## 构造决策树

`递归地选择最优特征`（能将数据集往“纯度”更高的方向划分），依次直到不能划分为止（所有子集被基本正确分类或者没有合适特征为止）。

这个过程是**局部最优**的，因为每个节点的划分都是选择当前条件下最好的分类，没有考虑后面的节点划分情况，从全局看未必是最优，仅是次优解，并且这种启发式的贪心算法，容易产生`过拟合`，模型泛化能力差。

但，我们可以从下往上对决策树进行`剪枝`，剪去过于细分的叶节点，回退到父节点并更改为叶节点，提升模型的泛化能力，这也是考虑**全局最优**的方式。

> 关键问题：如何选择合适的特征以及该特征对应的切分位置

## 切分点查找、评估方式

1. 信息增益（熵减） Information Gain (Entropy Decrease)

信息增益 = 父类信息熵 - 按某特征分割后子类信息熵

![20220107235949](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220107235949.png)

举例：

![20220108000446](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220108000446.png)

每次选择信息增益最大的特征作为切分特征。
