<!--
 * @Description: 
 * @Version: 1.0
 * @Autor: xihuishaw
 * @Date: 2022-01-07 23:42:54
 * @LastEditors: xihuishaw
 * @LastEditTime: 2022-01-09 11:25:12
-->

# 决策树

决策树学习的本质是，**从训练数据集中归纳出一组分类规则**。

## 构造决策树

`递归地选择最优特征`（能将数据集往“纯度”更高的方向划分），依次直到不能划分为止（所有子集被基本正确分类或者没有合适特征为止）。

这个过程是**局部最优**的，因为<u>每个节点的划分都是选择当前条件下最好的分类</u>，没有考虑后面的节点划分情况，从全局看未必是最优，仅是次优解，并且这种启发式的贪心算法，容易产生`过拟合`，模型泛化能力差。

但，我们可以从下往上对决策树进行`剪枝`（pruning），剪去过于细分的叶节点，回退到父节点并更改为叶节点，提升模型的泛化能力，这也是考虑**全局最优**的方式。

> 关键问题：如何选择合适的特征以及该特征对应的切分位置

## 切分点查找、评估方式

特征选择的准则：该特征具有很强的`分类能力`。

### 1、信息增益（Information Gain）

也叫熵减， Entropy Decrease，熵是描述信息不确定状态的一种度量，值越大则越不稳定。

信息增益 = 父节点信息熵 - 按某特征分割后子节点信息熵（条件熵）

![20220107235949](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220107235949.png)

举例：

![20220108000446](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220108000446.png)

递归选择信息增益最大的特征作为切分特征。

但是，如果遇到那种uuid、编号等特征，模型很容易将一个id处理成一类，此时条件熵往往等于0，信息增益肯定最大，这里就会陷入特征选择的误区。这并不是我们想要的分类，容易过拟合。

所以说，`信息增益更偏好于选择取值较多的特征`。

### 2、信息增益比(Gain Ratio)

信息增益比可以很好的校正信息增益的问题。分子还是信息增益，分母为一个惩罚项，即特征不同取值下的熵之和。

![20220108010147](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220108010147.png)

当分裂数很多的时候（比如一个id对应着一个类），惩罚项会变大，分裂算法可有效避免选择这类特征做分裂。

#### 小结

「信息增益」不能完全反应一个「分裂」的有效性， 通过改造使用「信息增益比」公式，可以有效避免决策树分裂策略使用类似「uuid」、「编号」类的取值量特别大的特征，陷入「最大化信息增益」的误区中。

特征分裂应该使用「二叉树」还是「多叉树」也可以从这个例子中得到启发，二叉树的特性使它自带抑制「过拟合」的功能，毕竟二叉树不能一下子直接分完，多叉树是可能一次性直接将样本分完。

![20220108012123](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220108012123.png)

## 决策树生成算法

1. ID3 ：信息增益
2. C4.5：信息增益比
3. CART

### CART

CART,classification and regression tree,分类回归树，既可以用于分类也可以用于回归。

CART采用的二叉树递归，每个非叶子节点都有2个分支。

- 当CART是`分类树`的时候，采用`GINI值`作为分裂节点的依据；
- 当CART作为`回归树`的时候，使用样本的`最小方差`作为分裂节点的依据。

#### 1、回归树

最小二乘法 回归树生成算法：

选择某特征，将所有取值分为2类（分类有很多种，为1：n），计算2类对应的y的均值c，并计算2类取值的均方误差（yi-c)^2，遍历所有的分类情况，选择误差最小的切分情况。

![20220109005231](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109005231.png)

示例:

![20220109112049](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109112049.png)

#### 2、分类树

分类树是用`基尼指数`选择最优特征，同时决定该特征的最优二值切分点。<u>基尼系数越小，不纯度越低，特征越好。</u>

![20220109112335](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109112335.png)

---

参考：

1. <https://blog.csdn.net/gzj_1101/article/details/78355234>
2. <https://www.cnblogs.com/keye/p/10564914.html>
3. <https://zhuanlan.zhihu.com/p/85731206>
