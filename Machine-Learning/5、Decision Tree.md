<!--
 * @Description: Decision Tree
 * @Version: 1.0
 * @Autor: xihuishaw
 * @Date: 2022-01-07 23:42:54
 * @LastEditors: xihuishaw
 * @LastEditTime: 2022-01-09 23:59:00
-->

# 决策树

决策树的思想是，用节点代表样本集合，通过某些判定条件来对节点内的样本进行分配，将它们划分到该节点下的子节点，并且要求各个子节点中类别的纯度之和应高于该节点中的类别纯度，从而起到分类效果。

决策树学习的本质是，**从训练数据集中归纳出一组分类规则**。

## 构造决策树

`递归地选择最优特征`（能将数据集往“纯度”更高的方向划分），依次直到不能划分为止（所有子集被基本正确分类或者没有合适特征为止）。

这个过程是**局部最优**的，因为<u>每个节点的划分都是选择当前条件下最好的分类</u>，没有考虑后面的节点划分情况，从全局看未必是最优，仅是次优解，并且这种启发式的贪心算法，容易产生`过拟合`，模型泛化能力差。

但，我们可以从下往上对决策树进行`剪枝`（pruning），剪去过于细分的叶节点，回退到父节点并更改为叶节点，提升模型的泛化能力，这也是考虑**全局最优**的方式。

> 关键问题：如何选择合适的特征以及该特征对应的切分位置

## 切分点查找、评估方式

特征选择的准则：该特征具有很强的`分类能力`。

### 1、信息增益（Information Gain）

也叫熵减， Entropy Decrease，[熵](https://zhuanlan.zhihu.com/p/26486223)是描述信息不确定状态的一种度量，值越大则越不稳定（事件发生的概率越低，所包含的信息量越大）。

信息增益 = 父节点信息熵 - 按某特征分割后子节点`条件熵`

![20220107235949](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220107235949.png)

其中条件熵指的是，节点分裂之后带来了多少不确定性的降低或纯度的提高：

![20220109164713](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109164713.png)

在每一个小类里面，都计算一个小熵，然后每一个小熵乘以各个类别的概率，然后求和。

所以，信息增益代表随机变量X的取值，降低了Y的不确定性的平均减少量。

举例：

![20220108000446](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220108000446.png)

递归选择信息增益最大的特征作为切分特征。

但是，如果遇到那种uuid、编号等特征，模型很容易将一个id处理成一类，此时条件熵往往等于0（$H(Y\vert X)=0$），信息增益肯定最大，这里就会陷入特征选择的误区。这并不是我们想要的分类，容易过拟合。

所以说，`信息增益更偏好于选择取值较多的特征`。

### 2、信息增益比(Gain Ratio)

信息增益比可以很好的校正信息增益的问题。分子还是信息增益，分母为一个惩罚项，即特征不同取值下的熵之和。

在类别占比均匀的情况下，类别数越多则熵越高，因此可以使用特征对应的熵来进行惩罚，即`熵越高的变量会在信息增益上赋予更大程度的抑制`。

![20220108010147](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220108010147.png)

当分裂数很多的时候（比如一个id对应着一个类），惩罚项会变大，分裂算法可有效避免选择这类特征做分裂。

#### 小结

「信息增益」不能完全反应一个「分裂」的有效性， 通过改造使用「信息增益比」公式，可以有效避免决策树分裂策略使用类似「uuid」、「编号」类的取值量特别大的特征，陷入「最大化信息增益」的误区中。

特征分裂应该使用「二叉树」还是「多叉树」也可以从这个例子中得到启发，二叉树的特性使它自带抑制「过拟合」的功能，毕竟二叉树不能一下子直接分完，多叉树是可能一次性直接将样本分完。

![20220108012123](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220108012123.png)

## 决策树生成算法

1. ID3 ：**信息增益**
   - 没有剪枝策略，容易过拟合；
   - 信息增益准则`对可取值数目较多的特征有所偏好`，类似“编号”的特征其信息增益接近于 1；
   - 只能用于处理离散分布的特征;
   - 没有考虑缺失值;
2. C4.5：**信息增益比**
   （1）处理数值特征：连续特征离散化，假设 n 个样本的连续特征 A 有 m 个取值，C4.5 将其排序并取相邻两样本值的平均数共 m-1 个划分点，分别计算以该划分点作为二元分类点时的信息增益，并选择信息增益最大的点作为该连续特征的二元离散分类点（`最佳分割法`，对应了sklearn中splitter参数的best选项）；
   （2）处理含缺失值的特征：`样本的缺失值占比越大，那就对信息增益的惩罚就越大`，这是因为缺失值本身就是一种不确定性成分。节点N的样本缺失值比例为γ，修正后的信息增益为：

   ![20220109224742](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109224742.png)

   - C4.5 只能用于分类;
   - C4.5 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算；
   - C4.5 在构造树的过程中，对数值属性值需要按照其大小进行排序，从中选择一个分割点，所以只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时，程序无法运行；
1. CART
   CART 算法的二分法可以简化决策树的规模，提高生成决策树的效率。

### CART

CART,classification and regression tree,分类回归树，既可以用于分类也可以用于回归。

CART采用的二叉树递归，每个非叶子节点都有2个分支。

- 当CART是`分类树`的时候，采用`GINI值`作为分裂节点的依据；
- 当CART作为`回归树`的时候，使用样本的`最小方差`作为分裂节点的依据。

#### 1、回归树

回归树节点纯度：`节点间元素大小越接近则纯度越高`，因此可以考虑使用均方误差（MSE）或平均绝对误差（MAE）来替换熵和条件熵的位置。

最小二乘法 回归树生成算法：

选择某特征，将所有取值分为2类（分类有很多种，为1：n），计算2类对应的y的均值c，并计算2类取值的均方误差（yi-c)^2，遍历所有的分类情况，选择误差最小的切分情况。

![20220109005231](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109005231.png)

示例:

![20220109112049](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109112049.png)

#### 2、分类树

分类树是用`基尼指数`选择最优特征，同时决定该特征的最优二值切分点。<u>基尼系数越小，不纯度越低，特征越好。</u>

补充：由于对数函数log的计算代价较大，CART将熵中的log在p=1处利用一阶泰勒展开，`基尼系数定义为熵的线性近似`。

![20220109112335](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109112335.png)

示例：

![20220109141753](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109141753.png)

#### 3、剪枝

- **预剪枝**，是指树在判断节点是否分裂的时候就预先通过一些规则来阻止其分裂;
- **后剪枝**，是指在树的节点已经全部生长完成后，通过一些规则来摘除一些子树。

CART采用后剪枝法，即先生成决策树，然后产生所有剪枝后的CART树，然后使用交叉验证检验剪枝的效果，选择泛化能力最好的剪枝策略。

我们希望减少树的大小来防止过拟合（降低树的复杂度），但又担心去掉节点后预测误差会增大，所以定义一个损失函数来达到这两个变量之间的平衡。

损失函数定义如下：

![20220109142130](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109142130.png)

C(T)为预测误差，|T|为子树T的叶子节点数（代表树的复杂度）

##### 补充

在sklearn的CART实现中，一共有6个控制预剪枝策略的参数，它们分别是：

- 最大树深度-max_depth
- 节点分裂的最小样本数-min_samples_split
- 叶节点最小样本数-min_samples_leaf
- 节点样本权重和与所有样本权重和之比的最小比例-min_weight_fraction_leaf
- 最大叶节点总数-max_leaf_nodes
- 分裂阈值-min_impurity_decrease

## 总结

- 划分标准的差异

  ID3 使用信息增益偏向特征值多的特征，C4.5 使用信息增益率克服信息增益的缺点，偏向于特征值小的特征，CART 使用基尼指数克服 C4.5 需要求 log 的巨大计算量，偏向于特征值较多的特征。
- 使用场景的差异
  
  ID3 和 C4.5 都只能用于分类问题，CART 可以用于分类和回归问题；ID3 和 C4.5 是多叉树，速度较慢，CART 是二叉树，计算速度很快；
- 样本数据的差异
  
  ID3 只能处理离散数据且对缺失值敏感，C4.5 和 CART 可以处理连续性数据且有多种方式处理缺失值；从样本量考虑的话，小样本建议 C4.5、大样本建议 CART。C4.5 处理过程中需对数据集进行多次扫描排序，处理成本耗时较高，而 CART 本身是一种大样本的统计方法，小样本处理下泛化误差较大 ；
- 样本特征的差异
  
  ID3 和 C4.5 层级之间只使用一次特征，CART 可多次重复使用特征；
- 剪枝策略的差异
  
  ID3 没有剪枝策略，C4.5 是通过悲观剪枝策略来修正树的准确性，而 CART 是通过代价复杂度剪枝。

## 思考

1. ID3树算法、C4.5树算法和CART算法之间有何异同？
   ![20220109232019](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220109232019.png)

2. 什么是信息增益？它衡量了什么指标？它有什么缺陷？
   - 信息增益，即节点分裂之后带来了多少不确定性的降低或纯度的提高。

   - 一般来说，通过一种划分方式带来的纯度提升越大，信息增益就越高。ID3算法以信息增益为准则来选择决策树划分属性。值多的属性更有可能会带来更高的纯度提升，所以信息增益的比较偏向选择取值多的属性。

   - 可能会带来一个不好的结果，如果选择唯一ID作为划分属性，那么会得到n个类别，每个类别都只包含一个样本，每个节点的纯度都是最高的，纯度提升也是最大的，带来的信息增益也是最高的。但是这样的划分是没有意义的。

3. sklearn决策树中的random_state参数控制了哪些步骤的随机性？
   - random_state参数主要是为了`保证每次都分割一样的训练集和测试集`，大小可以是任意一个整数，在调参的时候，只要保证其值一致即可。

   - 因为同一算法模型在不同的训练集和测试集的会得到不同的准确率，无法调参。所以在sklearn 中可以通过添加random_state，通过固定random_state的值，每次可以分割得到同样训练集和测试集。

4. 决策树如何处理连续变量和缺失变量？
   - 处理连续变量：处理连续型属性（例如西瓜的成熟度、学生成绩）时，需要将其离散化，将连续型属性的值划分到不同的区间（类似于二叉排序树），比较各个分裂点的Gain值的大小；
   - 处理缺失值
    1. 在选择分裂属性的时候，训练样本存在缺失值
        假如使用ID3算法，那么选择分类属性时，就要计算所有属性的熵增(信息增益，Gain)。假设10个样本，属性是a,b,c。在计算a属性熵时发现，第10个样本的a属性缺失，那么就把第10个样本去掉，前9个样本组成新的样本集，在新样本集上按正常方法计算a属性的熵增。然后结果乘0.9（新样本占raw样本的比例），就是a属性最终的熵。(`无缺失值样本所占的比例乘以无缺失值样本子集的信息增益`)
    1. 分类属性选择完成，训练样本属性缺失
        将缺失值样本按不同的概率划分到了所有分支中，而概率则等于无缺失值样本在每个分支中所占的比例。
    2. 决策树构造完成后，如果测试样本的属性值不完整
        测试样本在该属性值上有缺失值，就同时计算所有分支，然后算每个类别的概率，取概率最大的类别赋值给该样本。

5. 基尼系数是什么？为什么要在CART中引入它？
   在ID3算法中我们使用了信息增益来选择特征，信息增益大的优先选择。在C4.5算法中，采用了信息增益比来选择特征，以减少信息增益容易选择特征值多的特征的问题。但是无论是ID3还是C4.5,都是基于信息论的熵模型的，这里面会**涉及大量的对数运算**。所以`为了简化模型的同时也不至于完全丢失熵模型，CART分类树算法使用基尼系数来代替信息增益比`，基尼系数代表了模型的不纯度，基尼系数越小，则不纯度越低，特征越好。这和信息增益(比)是相反的。

6. 什么是树的预剪枝和后剪枝？具体分别是如何操作的？
   - 树的剪枝就是剪掉树的一些枝叶，考虑大决策树的枝代表着逻辑判断，也代表着分类后的子集。`决策树的剪枝就是删掉一些不必要的逻辑判断，并且将子集合并`。这样确实会造成在训练集上子集不纯的现象，但是因为我们最终目标是模型在测试集上的效果，所以**牺牲在训练集上的效果换取解决测试集的过拟合问题**，这样的做法也是值得的。

   - 决策树剪枝可以分为两类，预剪枝(Pre-Pruning)和后剪枝(Post-Pruning)。
      - PrePrune：预剪枝，及早的停止树增长；
      - PostPrune：后剪枝，在已生成过拟合决策树上进行剪枝，可以得到简化版的剪枝决策树。

---

参考：

1. [CART算法的原理以及实现](<https://blog.csdn.net/gzj_1101/article/details/78355234>)
2. [CART分类树](<https://www.cnblogs.com/keye/p/10564914.html>)
3. [【机器学习】决策树（上）——ID3、C4.5、CART](<https://zhuanlan.zhihu.com/p/85731206>)
4. [通俗理解信息熵](https://zhuanlan.zhihu.com/p/26486223)
5. [通俗理解条件熵](https://zhuanlan.zhihu.com/p/26551798)
6. 《统计学习方法》
7. <https://datawhalechina.github.io/ML-FTTI/01_tree_ensemble/01_tree.html>
