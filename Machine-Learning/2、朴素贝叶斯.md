<!--
 * @Description: 
 * @Version: 1.0
 * @Autor: xihuishaw
 * @Date: 2021-12-11 17:15:06
 * @LastEditors: xihuishaw
 * @LastEditTime: 2021-12-12 00:51:36
-->

# 朴素贝叶斯

在概率论与统计学中，贝叶斯定理 (Bayes' theorem) 表达了一个事件发生的概率，而确定这一概率的方法是基于与该事件相关的条件先验知识 (prior knowledge)。而利用相应先验知识进行概率推断的过程为贝叶斯推断 (Bayesian inference)。

## 贝叶斯公式

贝叶斯公式是由条件概率推导而来：

![20211211232812](https://s2.loli.net/2021/12/11/1BVLKYOQ8JNpI9l.png)

## 贝叶斯推断

P(A)称为"先验概率"（Prior probability），即在B事件发生之前，对A事件概率的一个判断。

P(A|B)称为"后验概率"（Posterior probability），即在B事件发生之后，对A事件概率的重新评估。

P(B|A)/P(B)称为"可能性函数"（Likelyhood），这是一个调整因子，使得预估概率更接近真实概率。

so，`后验概率＝先验概率 ｘ 调整因子`·

>先预估一个"先验概率"，然后加入实验结果，看这个实验到底是增强还是削弱了"先验概率"，由此得到更接近事实的"后验概率"。

## “朴素”的含义

换另一个角度看贝叶斯公式：

一般在监督学习中，训练数据的类别（label）出现的概率，每个特征取值的概率以及在每个类别（label）对应的特征取值的概率，都是已知的，代入贝叶斯公式，就能得出在已知特征条件下的类别概率。

![20211211172101](https://s2.loli.net/2021/12/11/BmwH9ZDXQpV4r7n.png)

思想基础：

以`自变量之间的独立（条件特征独立）性`和`连续变量的正态性假设`为前提：

对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别c

![20211212001903](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20211212001903.png)

举例，比如下面2个类别，取最终概率最大的类别作为输出类别：

![20211211172938](https://s2.loli.net/2021/12/11/eJ8dIvlMjic4gW6.png)

![20211212004757](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20211212004757.png)

再从数据角度通俗理解：

>对于一条测试集数据来说，特征是确定的，也就是分母是一定的，现在就是想让分子最大化，最后预测类别的概率才最大；
>
>对于分子，根据训练集数据可知，p（类别）是已知的，不同类别下的不同特征的条件概率p（特征|类别）也是可得知的。
>
>目标：计算**在不同类别下，使p（类别）×p（特征|类别）最大化**，这个过程需使用`极大似然估计（MLE)`来估计对应概率。

## 算法优缺点

1. 在实际场景里，很难满足各个特征之间独立；在属性相关性较小时，朴素贝叶斯性能良好；

---

参考：

1. <https://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html>

2. <https://www.cnblogs.com/geo-will/p/10468401.html>

3.
