# KNN

## 定义

k近邻法(k-nearest neighbor, k-NN)

原理：样本中每个数据都存在标签（每个数据的分类都清楚~），输入新的数据后，`计算所有样本与新数据的距离，选择最近的K个样本，其中出现次数最多的分类，即为新数据的分类`。

## 算法步骤

1. 计算已知类别数据集中的点与当前点之间的距离；
2. 按照距离递增次序排序；
3. 选取与当前点距离最小的k个点；
4. 确定前k个点所在类别的出现频率；
5. 返回前k个点所出现频率最高的类别作为当前点的预测分类。

## scikit-learn

KNneighborsClassifier参数说明：

1. n_neighbors：默认为5，就是k-NN的k的值，选取最近的k个点。

2. weights：默认是`uniform`，参数可以是uniform、distance，也可以是用户自己定义的函数。

- `uniform，是均等的权重`，即所有邻近点的权重相等。
- distance，是不均等的权重，`距离近的点比距离远的点的影响大`。
- 用户自定义的函数，接收距离的数组，返回一组维数相同的权重。

3. algorithm：快速k近邻搜索算法，默认参数为auto，可以理解为算法自己决定合适的搜索算法。除此之外，用户也可以自己指定搜索算法ball_tree、kd_tree、brute方法进行搜索。

- brute是暴力搜索，也就是线性扫描，当训练集很大时，计算非常耗时。
- kd_tree，就是构造kd树存储数据，一种对其进行快速检索的树形数据结构，其实就是数据结构中的二叉树。以中值切分构造的树，每个结点是一个超矩形，`在维数小于20时效率高`。
- ball tree是为了克服kd树高维失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体。

4. leaf_size：默认是30，这个是构造的kd树和ball树的大小。该值会影响树构建的速度和搜索速度，同样也影响着存储树所需的内存大小。需根据问题性质选择最优大小。

5. metric：用于距离度量，默认度量是minkowski，也就是p=2的欧氏距离(欧几里德度量)。

6. p：距离度量公式

- =1，曼哈顿距离
- =2，欧氏距离

7. n_jobs：并行处理设置。默认为1，临近点搜索并行工作数。如果为-1，那么CPU的所有cores都用于并行工作。

## 算法优缺点

优点

- 简单，易理解，精度高，既可以用来做`分类`也可以用来做`回归`；
- 可用于`数值型数据和离散型数据`；
- 训练时间复杂度为O(n)，无数据输入假定；
- `对异常值不敏感`；

缺点

- 对数据集中的每个数据都计算距离值，使用可能非常耗时;
- 保存全部数据集，如果训练数据集很大，必须使用大量的存储空间；
- **样本不平衡问题**（即有些类别的样本数量很多，而其它样本的数量很少）；
- 最大的缺点是无法给出数据的内在含义。
