<!--
 * @Description: 
 * @Version: 1.0
 * @Autor: xihuishaw
 * @Date: 2022-01-03 00:28:01
 * @LastEditors: xihuishaw
 * @LastEditTime: 2022-01-03 01:19:54
-->

## 梯度向量

梯度：
表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）

$$
\operatorname{gradf}(\mathrm{x}, \mathrm{y})=\nabla f(x, y)=\left\{\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}\right\}=f_{x}(x, y) \bar{i}+f_{y}(x, y) \bar{j}
$$

梯度向量：

定义：目标函数f为单变量，是关于`自变量向量x`=(x1,x2,…,xn)T的函数，

单变量函数f对向量x求梯度，结果为一个与向量x同维度的向量，称之为梯度向量；

## 雅克比矩阵(Jacobian矩阵)

定义：目标函数f为一个函数向量，f=(f1(x),f2(x),…fm(x))T;

其中，自变量x=(x1,x2,…,xn)T；

函数向量f对x求梯度，结果为一个矩阵；行数为f的维数；列数位x的维度，称之为Jacobian矩阵；`其每一行都是由相应函数的梯度向量转置构成的`；

![20220103005140](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220103005140.png)

## 海森矩阵(Hessian 矩阵)

海森矩阵（Hessian matrix 或 Hessian）是`一个自变量为向量的实值函数的二阶偏导数`组成的方块矩阵。

>实际上，Hessian矩阵是梯度向量g(x)对自变量x的Jacobian矩阵（梯度向量对自变量x的二阶偏导）。

![20220103005540](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220103005540.png)



## 带等式约束的优化问题--拉格朗日乘子法

1. 等式约束条件；
2. 目标；
3. 拉个朗日函数；
4. 对拉格朗日函数求偏导；

![20220103010133](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220103010133.png)

## 泰勒公式

用一个多项式函数去逼近一个给定的函数(即尽量使多项式函数图像拟合给定的函数图像)。

如果一个非常复杂函数，想求其某点的值，直接求无法实现，这时候可以使用泰勒公式去近似的求该值，这是泰勒公式的应用之一（近似求解）。

定义：设 $n$ 是一个正整数。如果定义在一个包含a的区间上的函数 $f$ 在 $a$ 点处 $n+1$ 次可导，那么对于这个区间上的任意 $x$ 都有:
$$
\begin{array}{c}
f(x)=\frac{f(a)}{0 !}+\frac{f^{\prime}(a)}{1 !}(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^{2}+\cdots+\frac{f^{(n)}(a)}{n !}(x-a)^{n}+R_{n}(x) \\
=\sum_{n=0}^{N} \frac{f^{(n)}(a)}{n !}(x-a)^{n}+R_{n}(x)
\end{array}
$$

**泰勒公式在机器学习中主要应用于梯度迭代**

## 基于梯度的优化方法--梯度下降法

```python
import numpy as np
import matplotlib.pyplot as plt

# 函数
def f(x):
    return np.power(x, 2)

# x^2求导
def d_f_1(x):
    '''
    求导数的方式1
    '''
    return 2.0 * x

def d_f_2(f, x, delta=1e-4):
    '''
    求导数的第二种方法,根据导数的定义
    '''
    return (f(x+delta) - f(x-delta)) / (2 * delta)


# plot the function
xs = np.arange(-10, 11)
plt.plot(xs, f(xs))


learning_rate = 0.1
max_loop = 30

x_init = 10.0
x = x_init
lr = 0.1
x_list = []
for i in range(max_loop):
    #d_f_x = d_f_1(x)
    d_f_x = d_f_2(f, x) # 计算梯度
    x = x - learning_rate * d_f_x # 更新x
    x_list.append(x)
x_list = np.array(x_list)
plt.scatter(x_list,f(x_list),c="r")
plt.show()

print('initial x =', x_init)
print('arg min f(x) of x =', x)
print('f(x) =', f(x))
```

## 基于梯度的优化方法--牛顿迭代法

利用牛顿法求解目标函数的最小值其实是转化成`求使目标函数的一阶导为0的参数值`。这一转换的理论依据是，函数的极值点处的一阶导数为0。

其迭代过程是在当前位置x0求该函数的切线，该切线和x轴的交点x1，作为新的x0,重复这个过程，直到交点和函数的零点重合。此时的参数值就是使得目标函数取得极值的参数值。

![20220103011825](https://cdn.jsdelivr.net/gh/xihuishawpy/PicBad@main/blogs/pictures/20220103011825.png)

---
参考：
1. <https://blog.csdn.net/liuliqun520/article/details/80019507>